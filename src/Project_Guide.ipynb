{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide Notebook\n",
    "\n",
    "This notebook is designed to provide a guide for the replication project with sample of the data structure, key functionalities of our code, challenges we faced and the result of replication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from pathlib import Path\n",
    "OUTPUT_DIR = Path(config.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetch:\n",
    "\n",
    "There are two key data features we fetch. \n",
    "\n",
    "1) CDS Rates: Source: Markit on WRDS\n",
    "\n",
    "2) Risk free rates: Fred website and FED website - \n",
    "\n",
    "        3-Month Treasury Constant Maturity Rate: https://fred.stlouisfed.org/series/DGS3MO\n",
    "        6-Month Treasury Constant Maturity Rate: https://fred.stlouisfed.org/series/DGS6MO \n",
    "        Swap Rates: https://www.federalreserve.gov/data/yield-curve-tables/feds200628_\n",
    "\n",
    "The data fetch process is automated. The CDS rates are fetched via WRDS queries to the SAS database server that hosts Markit tables. \n",
    "\n",
    "The risk free rates are fetched from the websites using pandas webreader. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDS Data\n",
    "\n",
    "The below snippet shows how we fetch the cds data from Markit. \n",
    "\n",
    "This fetched data has close to 6000 tickers. We fetch the parspread for each day available from 2001 to 2024. The paper uses data until 2016, however we extend to the whole period available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](../assets/snip_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Steps:\n",
    "\n",
    "The paper states 20 CDS portoflios are created, however it does not offer details on the construction of the 20 portfolios using the 6000 tickers available. \n",
    "\n",
    "We propose the following process based on our research of the CDS returns calculation and He Kelly Manela's paper. This methodology is in consultation with the Professor Jeremey. \n",
    "\n",
    "1) Since the returns is calculated on a monthly basis, we resample the CDS data to monthly \n",
    "\n",
    "2) We propose to construct the 20 portfolios by splitting the set of CDS monthly rates for the 6000 tickets into 20 quantiles. This ensures a monthly rebalancing into quantiles. \n",
    "\n",
    "3) Once we have all 6000 tickers into 20 different quantiles for each month, we obtain one single value for each quantile which will form the CDS spread value for that particular portfolio. \n",
    "\n",
    "4) We combine the CDS spreads for all tickers within a quantile through three approaches and try all of them to see which might work best:\n",
    "\n",
    "        a) Mean: A simple mean of the spreads within each portfolio \n",
    "        b) Median: Probably a more accurate representation of CDS returns within a portfolio of CDS products.\n",
    "        c) Weighted: Calculate a weighted mean of the CDS spreasds within each portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges: \n",
    "\n",
    "1) Lack of clarity on portfolio construction. We do not have a definite method of how the CDS spreads of all tickers are combined to form 20 different CDS portfolios. Fix: Try out multiple methods on constructing the portfolio. \n",
    "\n",
    "2) High volatility in 20th quantile. We notice very high volaitlity in the 20th quantile of CDS spreads. Although we expected the 20th quantile to be more volatile than the other 19 because the quantiles are constructed in such a manner, the values we noticed were notoriously high. Fix: Implement a smoothening method on the 20th quantile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
