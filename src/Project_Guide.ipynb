{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide Notebook\n",
    "\n",
    "This notebook is designed to provide a guide for the replication project with sample of the data structure, key functionalities of our code, challenges we faced and the result of replication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from pathlib import Path\n",
    "OUTPUT_DIR = Path(config.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetch:\n",
    "\n",
    "There are two key data features we fetch. \n",
    "\n",
    "1) CDS Rates: Source: Markit on WRDS\n",
    "\n",
    "2) Risk free rates: Fred website and FED website - \n",
    "\n",
    "        3-Month Treasury Constant Maturity Rate: https://fred.stlouisfed.org/series/DGS3MO\n",
    "        6-Month Treasury Constant Maturity Rate: https://fred.stlouisfed.org/series/DGS6MO \n",
    "        Swap Rates: https://www.federalreserve.gov/data/yield-curve-tables/feds200628_\n",
    "\n",
    "The data fetch process is automated. The CDS rates are fetched via WRDS queries to the SAS database server that hosts Markit tables. \n",
    "\n",
    "The risk free rates are fetched from the websites using pandas webreader. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDS Data\n",
    "\n",
    "The below snippet shows how we fetch the cds data from Markit. \n",
    "\n",
    "This fetched data has close to 6000 tickers. We fetch the parspread for each day available from 2001 to 2024. The paper uses data until 2016, however we extend to the whole period available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](../assets/snip_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Steps:\n",
    "\n",
    "The paper states 20 CDS portoflios are created, however it does not offer details on the construction of the 20 portfolios using the 6000 tickers available. \n",
    "\n",
    "We propose the following process based on our research of the CDS returns calculation and He Kelly Manela's paper. This methodology is in consultation with the Professor Jeremey. \n",
    "\n",
    "1) Since the returns is calculated on a monthly basis, we resample the CDS data to monthly \n",
    "\n",
    "2) We propose to construct the 20 portfolios by splitting the set of CDS monthly rates for the 6000 tickets into 20 quantiles. This ensures a monthly rebalancing into quantiles. \n",
    "\n",
    "3) Once we have all 6000 tickers into 20 different quantiles for each month, we obtain one single value for each quantile which will form the CDS spread value for that particular portfolio. \n",
    "\n",
    "4) We combine the CDS spreads for all tickers within a quantile through three approaches and try all of them to see which might work best:\n",
    "\n",
    "        a) Mean: A simple mean of the spreads within each portfolio \n",
    "        b) Median: Probably a more accurate representation of CDS returns within a portfolio of CDS products.\n",
    "        c) Weighted: Calculate a weighted mean of the CDS spreasds within each portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges: \n",
    "\n",
    "1) Lack of clarity on portfolio construction. We do not have a definite method of how the CDS spreads of all tickers are combined to form 20 different CDS portfolios. Fix Implemented: Try out multiple methods on constructing the portfolio. \n",
    "\n",
    "2) High volatility in 20th quantile. We notice very high volaitlity in the 20th quantile of CDS spreads. Although we expected the 20th quantile to be more volatile than the other 19 because the quantiles are constructed in such a manner, the values we noticed were notoriously high. Fix Implemented: Implement a smoothening method on the 20th quantile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](../assets/cds_box_plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](../assets/cds_methods.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'cds_processing.py' returns a dataframe containing the returns based on one of the three methods as specified by the user. As a default method, we use the Median technique as it seemed most intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process description: \n",
    "\n",
    "1) cds_data_fetch.py - This file is soleley to fetch data for CDS spreads from WRDS. \n",
    "\n",
    "    a) get_cds_data() - Establishes a database connection to WRDS, extracts 5Y tenor CDS data for the years 2001 - 2024.\n",
    "\n",
    "2) cds_processing.py - This file uses the fetched data from the previous file and processes the raw data into a format that will be uses for calculations. \n",
    "\n",
    "    a) Auxiliary function - assign_quantiles() and resample_end_of_month() cuts data into quantiles and resample the data as per end of month timestamp respectively. \n",
    "\n",
    "    b) process_cds_data() - Cleans the rawdataframe and aligns the data in a readable fashion. All tickers for each month are grouped into different quantiles based on the spreads. \n",
    "\n",
    "    c) calc_cds_monthly() - Calculates the CDS monthly rate based on one of the three methods specified by the user. Default is set to median. \n",
    "    \n",
    "    d) process_cds_monthly() - Further processe the cds_20 series by treating outliers through a rolling median smoothening algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
